/*********************************************************************
 *
 * Description: C++/MPI proxy for Transformer-based models distributed training 
 *              with data parallelism
 * Author: Jacopo Raffi
 *
 *********************************************************************/

 //TODO: oneCCL support
#include <mpi.h>

#include <unistd.h>
#include <stdio.h>
#include <string>
#include <time.h>
#include <stdlib.h>
#include <assert.h>
#include <cstdint>
#include <cstdlib>
#include <filesystem>
#include <nlohmann/json.hpp>

#define NVML

namespace fs = std::filesystem;
using nlohmann::json;

// CCUTILS headers
#include <ccutils/mpi/mpi_timers.hpp>
#include <ccutils/mpi/mpi_macros.hpp>
#include <ccutils/macros.hpp>

#ifdef PROXY_ENABLE_CUDA
    #include <ccutils/cuda/cuda_macros.hpp>
#endif

#ifdef PROXY_ENABLE_HIP
    #include <ccutils/hip/hip_macros.hpp>
#endif

#ifdef PROXY_ENERGY_PROFILING
#include <profiler/power_profiler.hpp>
#endif

#ifdef PROXY_ENABLE_ONECCL
#include <oneapi/ccl.hpp>
#include <CL/sycl.hpp>
#endif

// Project headers
#include "../utils.hpp"
#include "../data_types.hpp"
#include "../proxy_classes.hpp"

#ifdef PROXY_ENABLE_NCCL
    #include <nccl.h>
Proxy_CommType world_comm;
#endif

#ifdef PROXY_ENABLE_RCCL
    #include <rccl.h>
Proxy_CommType world_comm;
#endif

// Device to use
#if defined(PROXY_ENABLE_CUDA) || defined(PROXY_ENABLE_HIP)
    constexpr Device device = Device::GPU;
#else
    constexpr Device device = Device::CPU;
#endif

// Default values
#define NUM_B 10
#define WARM_UP 8
#define RUNS 10
#define POWER_SAMPLING_RATE_MS 5

CCUTILS_MPI_TIMER_DEF(runtime)
CCUTILS_MPI_TIMER_DEF(barrier)

/**
 * @brief Simulates one iteration of data-parallel (using bucketing approach) training for a Transformer model.
 *
 * This function performs a mock forward pass and a backward pass with asynchronous
 * all-reduce operations for gradients over multiple parameter buckets.
 *
 * @param grad_ptrs Array of pointers to gradient buffers for each bucket.
 * @param sum_grad_ptrs Array of pointers to buffers storing the reduced gradients.
 * @param num_buckets Number of parameter buckets.
 * @param params_per_bucket Array containing the number of parameters in each bucket.
 * @param fwd_rt_whole_model Forward pass runtime in microseconds.
 * @param bwd_rt_per_B Backward pass runtime per bucket in microseconds.
 * @param comm Pointer to the Communicator object for Collective operations.
 * @return int Always returns 0.
 */
int run_data_parallel(Tensor<_FLOAT, device>** grad_ptrs, Tensor<_FLOAT, device>** sum_grad_ptrs, 
                    int num_buckets, uint64_t* params_per_bucket,
                    uint64_t fwd_rt_whole_model, float bwd_rt_per_B, ProxyCommunicator* communicator) {
    

    //forward compute
    usleep(fwd_rt_whole_model);
    //backward (idea is to overlap all-reduce with backward compute)

    int index, flag;
    for(int i=0; i<num_buckets; i++){
        usleep(bwd_rt_per_B); //compute backward of a bucket 
        communicator->Iallreduce(grad_ptrs[i]->data, sum_grad_ptrs[i]->data, params_per_bucket[i], i); //start all-reduce for the bucket
    }

    CCUTILS_MPI_TIMER_START(barrier)
    communicator->WaitAll(num_buckets); //wait for all all-reduce to complete
    CCUTILS_MPI_TIMER_STOP(barrier) 
    return 0;
}

int main(int argc, char* argv[]) {
    int rank, world_size;

    int num_buckets = NUM_B;
    if(argc < 4){
        std::cout << "Usage: mpirun -n <world_size> ./dp <model_name> <num_buckets> <base_path>\n";
        return -1;
    }

    std::string model_name = argv[1];
    if(argc > 2){
        num_buckets = std::stoi(argv[2]);
    }

    // --- Construct model stats file path ---
    fs::path repo_path = get_dnnproxy_base_path(argc, argv, rank);
    fs::path file_path = repo_path / "model_stats" / (model_name + ".txt");
    if (!fs::exists(file_path)) {
        std::cerr << "Error: model stats file does not exist: " << file_path << "\n";
        return -1;
    }

    std::map<std::string, uint64_t> model_stats = get_model_stats(file_path); // get model stats from file
    uint64_t fwd_rt_whole_model = model_stats["avgForwardTime"]; // in us
    float bwd_rt_per_B = (model_stats["avgBackwardTime"]) / num_buckets; // in us
    uint local_batch_size = model_stats["batchSize"];
    uint64_t total_model_size = model_stats["modelSize"]; // number of parameters
    
    uint64_t base_params_per_bucket = total_model_size / num_buckets;
    uint64_t remainder = total_model_size % num_buckets;
    uint64_t params_per_bucket[num_buckets];
    for (int i = 0; i < num_buckets; i++) {
        params_per_bucket[i] = base_params_per_bucket + (i < remainder ? 1 : 0); // distribute remainder across the buckets
    }
            
    MPI_Init(&argc,&argv);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    CCUTILS_MPI_INIT

#if defined(PROXY_ENABLE_CUDA)
    int num_gpus;
    cudaGetDeviceCount(&num_gpus);
    CCUTILS_CUDA_CHECK(cudaSetDevice(rank % num_gpus));
#elif defined(PROXY_ENABLE_HIP)
    int num_gpus;
    hipGetDeviceCount(&num_gpus);
    CCUTILS_HIP_CHECK(hipSetDevice(rank % num_gpus));
#endif

    #ifdef PROXY_ENABLE_CCL
    ncclUniqueId id;
    if (rank == 0) {
        ncclGetUniqueId(&id);
    }
    MPI_Bcast(&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);
    ncclCommInitRank(&world_comm, world_size, id, rank);
    CCLCommunicator* communicator = new CCLCommunicator(world_comm, num_buckets);
#elif defined(PROXY_ENABLE_ONECCL)
    // Select GPU devices
    std::vector<sycl::device> gpus = sycl::device::get_devices(sycl::info::device_type::gpu);
    int num_gpus = gpus.size();
    sycl::device dev = gpus[rank % num_gpus];
    sycl::context ctx(dev);
    sycl::queue queue(ctx, dev);

    // Initialize oneCCL
    ccl::init();

    // Create KVS (acts like ncclUniqueId)
    ccl::shared_ptr_class<ccl::kvs> kvs;
    if (rank == 0) {
        kvs = ccl::create_main_kvs();
    }

    // Serialize and broadcast KVS address via MPI
    std::vector<char> kvs_addr;
    if (rank == 0) kvs_addr = kvs->get_address();

    size_t addr_size = kvs_addr.size();
    MPI_Bcast(&addr_size, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);

    if (rank != 0) kvs_addr.resize(addr_size);
    MPI_Bcast(kvs_addr.data(), addr_size, MPI_BYTE, 0, MPI_COMM_WORLD);

    if (rank != 0) kvs = ccl::create_kvs(kvs_addr);

    // Create communicator
    auto world_comm_ccl = ccl::create_communicator(world_size, rank, kvs, ctx);
    OneCCLCommunicator* communicator = new OneCCLCommunicator(world_comm_ccl, num_buckets);    
#else
    MPICommunicator* communicator = new MPICommunicator(MPI_COMM_WORLD, MPI_FLOAT, num_buckets);
#endif

    Tensor<_FLOAT, device>* grad_ptrs[num_buckets];
    Tensor<_FLOAT, device>* sum_grad_ptrs[num_buckets];
    for(int i=0; i<num_buckets; i++){
        grad_ptrs[i] = new Tensor<_FLOAT, device>(params_per_bucket[i]);
        sum_grad_ptrs[i] = new Tensor<_FLOAT, device>(params_per_bucket[i]);
    }

    MPI_Barrier(MPI_COMM_WORLD);

    //warmup
    std::vector<float> energy_vals;
    for(int wmp = 0; wmp < WARM_UP; wmp++){
        run_data_parallel(grad_ptrs, sum_grad_ptrs, num_buckets, params_per_bucket,
                         fwd_rt_whole_model, bwd_rt_per_B, communicator);
    }
    std::string sub_folder = model_name + "_dp";

    // Add here the start and stop for the energy profiler
    #ifdef PROXY_ENABLE_NCCL
    std::string algo = getenv("NCCL_ALGO") ? getenv("NCCL_ALGO") : "default_algo";
    std::string proto = getenv("NCCL_PROTO") ? getenv("NCCL_PROTO") : "default_proto";
    std::string channels = getenv("NCCL_MAX_CTAS") ? getenv("NCCL_MAX_CTAS") : "default_ctas";
    std::string threads = getenv("NCCL_NTHREADS") ? getenv("NCCL_NTHREADS") : "default_threads";

    sub_folder += "_nccl_algo_" + algo + "_proto_" + proto + "_ctas_" + channels + "_threads_" + threads + "/";
    #endif
    std::string base_folder_path = "logs_" + std::to_string(world_size) + "/";
    for(int iter = 0; iter < RUNS; iter++){
        #ifdef PROXY_ENERGY_PROFILING
        std::string power_file = base_folder_path + sub_folder + "power_dp_rank_" + std::to_string(rank) + "run_" + std::to_string(iter) + ".csv";
        PowerProfiler powerProf(rank % num_gpus, POWER_SAMPLING_RATE_MS, power_file);
        #endif
        CCUTILS_MPI_TIMER_START(runtime)
        
        #ifdef PROXY_ENERGY_PROFILING
        powerProf.start();
        #endif
        run_data_parallel(grad_ptrs, sum_grad_ptrs, num_buckets, params_per_bucket,
                         fwd_rt_whole_model, bwd_rt_per_B, communicator);
        
        #ifdef PROXY_ENERGY_PROFILING
        powerProf.stop();
        float energy_consumed = powerProf.get_device_energy();
        energy_vals.push_back(energy_consumed);
        #endif
        CCUTILS_MPI_TIMER_STOP(runtime)
    }

    char host_name[MPI_MAX_PROCESSOR_NAME];
	char (*host_names)[MPI_MAX_PROCESSOR_NAME];
	int namelen,bytes,n,color;
	MPI_Get_processor_name(host_name,&namelen);

    std::vector<uint64_t> bucket_sizes(params_per_bucket,
                                   params_per_bucket + num_buckets);
    std::pair<float, float> msg_stats = compute_msg_stats(bucket_sizes, 1);

    CCUTILS_MPI_SECTION_DEF(dp, "Data Parallelism")
    float msg_size_avg = msg_stats.first;
    float msg_size_std = msg_stats.second;
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "model_name", model_name)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "num_buckets", num_buckets)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "local_batch_size", local_batch_size)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "world_size", world_size)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "fwd_rt_whole_model", fwd_rt_whole_model)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "bwd_rt_per_bucket", bwd_rt_per_B)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "total_model_size_params", total_model_size)
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "msg_size_avg_bytes", msg_size_avg*sizeof(_FLOAT))
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "msg_size_std_bytes", msg_size_std*sizeof(_FLOAT))
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "device", (device == Device::CPU) ? "CPU" : "GPU")
    CCUTILS_MPI_GLOBAL_JSON_PUT(dp, "backend", communicator->get_name())

    //erase warm-up elemements
    CCUTILS_SECTION_JSON_PUT(dp, "runtimes", __timer_vals_runtime);
    __timer_vals_barrier.erase(__timer_vals_barrier.begin(), __timer_vals_barrier.begin() + WARM_UP); // remove the warm-up barriers
    CCUTILS_SECTION_JSON_PUT(dp, "barrier_time", __timer_vals_barrier);
    CCUTILS_SECTION_JSON_PUT(dp, "hostname", host_name);

    #ifdef PROXY_ENERGY_PROFILING
    CCUTILS_SECTION_JSON_PUT(dp, "energy_consumed", energy_vals);
    #endif
    CCUTILS_MPI_SECTION_END(dp);

    #ifdef PROXY_ENABLE_CLL
    ncclCommDestroy(world_comm);
    #endif

    MPI_Finalize();
}
